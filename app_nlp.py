# -*- coding: utf-8 -*-
"""Untitled38.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eXX6RRpXpNu6WDJGQiSTRiWv0fIb_R5V
"""

#pip install scikit-multilearn

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from bs4 import BeautifulSoup
import string
import re 
from sklearn.multiclass import OneVsRestClassifier
from nltk.tokenize import word_tokenize
from skmultilearn.adapt import MLkNN
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline,Pipeline
import nltk
from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tag.perceptron import PerceptronTagger
from nltk.tag import pos_tag
from nltk.stem import SnowballStemmer
from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

def remove_punctuation(input):

  input='"' + input + '"'

  lang_dict={'c#':'csharp',
             'C#':'csharp',
             'c#-4.0':'csharp',

             'Go':'goLang',
             'go':'goLang',

             'VB':'VisualBasic',
             'vb':'VisualBasic',
             'Vb':'VisualBasic',
             'VB.net':'VisualBasic',

             'd3.js':'d3js',
             'd3\.js':'d3js',
             'nvd3.js':'d3js',

             'str':'string',

             'g++5.1':'gplusplus',
             'g++4.8':'gplusplus',
             'g++':'gplusplus',
             'gcc4.8':'gplusplus',
             'gcc':'gplusplus',
             'gcc10':'gplusplus',
             'g++10':'gplusplus',

             'c++':'cplusplus',
             'C++':'cplusplus',
             'c++14':'cplusplus',
             'C++14':'cplusplus',
             'c++17':'cplusplus',
             'C++17':'cplusplus',
             'c++98':'cplusplus',
             'C++98':'cplusplus',
             'c++11':'cplusplus',
             'C++11':'cplusplus',
             'c++20':'cplusplus',
             'C++20':'cplusplus',
             'clang++':'cplusplus',
             'libstdc++':'cplusplus',
             'libs++':'cplusplus',

             'c':'clang',
             'C':'clang',

             'qt':'Qt_framework',
             'qt4':'Qt_framework',
             'qt5':'Qt_framework',
             'qt-creator':'Qt_framework',
             'pyqt':'Qt_framework',

             'vue.js':'vuejs',
             'vuejs2':'vuejs',
             'vue-component':'vuejs',
             'vuex':'vuejs',
             'vuetify.js':'vuejs',
             'vue-router':'vuejs',

             'r':'rLang',
             'R':'rLang',

             '.net':'dotnet'}
  
  word_list=[]
  for word in input.split():
    if word in list(lang_dict.keys()):
      word_list.append(lang_dict[word])
    else:
      word_list.append(word)
  
  input=' '.join(token for token in word_list)
  #punctuation
  input=re.sub(pattern='\d',repl='',string=input) #Remove all digits
  input=re.sub(pattern='[^\w]',repl=' ',string=input) #Remove special characters
  input=re.sub(pattern=r'\s+[a-zA-Z]\s+',repl=' ',string=input) #Remove all single characters
  input=re.sub(pattern=r'\^[a-zA-Z]\s+',repl= ' ', string=input)#Remove single characters from the start
  
  return input



from nltk.corpus import stopwords
def remove_stopwords(input):
  stop_words=stopwords.words('english')
  stop_words.append(['how','why','is','what','use','can','keep','get','do'])
  return ' '.join(word for word in word_tokenize(input) if word not in stop_words)
  

def lemmatize(input,only_nouns=False):

  #main idea behind ''only_nouns'' argument is simplify the input with keeping only nouns for topic modeling 

  wnl=WordNetLemmatizer()
  word_lemmatized=[]
  for word,tag in pos_tag(word_tokenize(input)):

    if not only_nouns:
      if tag.startswith('N'):
        word_lemmatized.append(wnl.lemmatize(word,pos='n'))
      elif tag.startswith('V'):
        word_lemmatized.append(wnl.lemmatize(word,pos='v'))
      elif tag.startswith('J'):
        word_lemmatized.append(wnl.lemmatize(word,pos='a'))
      elif tag.startswith('R'):
        word_lemmatized.append(wnl.lemmatize(word,pos='r'))
      else:
        word_lemmatized.append(wnl.lemmatize(word,pos='n'))
    else:

      if tag.startswith('N'):
        word_lemmatized.append(wnl.lemmatize(word,pos='n'))    

  return ' '.join(x for x in word_lemmatized)

def title_pipeline(input):
  #clean_text
  input=remove_punctuation(input.lower())
  #text_mining
  input=lemmatize(remove_stopwords(input))

  return input

import pickle
model = pickle.load(open('finalized_model.sav', 'rb'))

import pickle
vectorizer = pickle.load(open('finalized_vect.sav', 'rb'))

import pickle
vectorizer2 = pickle.load(open('finalized_vect2.sav', 'rb'))

from flask import Flask,jsonify,make_response,request,render_template
import threading
app= Flask(__name__)

@app.route('/')
def home():
  return render_template('home.html')

@app.route('/tags',methods=['POST'])
def tags():
  if request.method == 'POST':
    text= request.form['message']
    cleaned_text=title_pipeline(text)
    text_vect=vectorizer.transform([cleaned_text])
    pred=model.predict(text_vect).toarray()
    pred_indexs=np.argsort(-pred)[:,:np.sum(pred)][0]
    tags_list=[vectorizer2.get_feature_names()[i] for i in pred_indexs]

  return render_template('result.html',prediction = tags_list)

if __name__=='__main__':
  app.run(debug=True)